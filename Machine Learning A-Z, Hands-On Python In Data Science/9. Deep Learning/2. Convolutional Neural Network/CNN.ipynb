{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b750d955-92e7-4b38-9e84-f70af95d7383",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23446c75-ad23-4766-85d6-fd8f9aac20e0",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4001a153-2fcc-4875-8fb6-0c84d7546b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 21:18:15.856316: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-24 21:18:15.856401: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cf6b74-dfbb-4747-8c61-f77ea851d9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ebe51-7b72-4635-a933-a92d40fc09d1",
   "metadata": {},
   "source": [
    "### Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a78890-a568-45c3-a80c-12e4751d90f4",
   "metadata": {},
   "source": [
    "#### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d845f9-17a3-48ca-be7f-18737731bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# we apply transformation a.k.a. IMAGE AUGMENTATION to trainging set to avoid the over-fitting\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set/',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b671cd-22d2-44a3-a544-98930ce513ce",
   "metadata": {},
   "source": [
    "#### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf81e70-ce68-4955-b493-eea00325caf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set/',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea48bb2-d10b-4d69-b2bd-07a9f608263a",
   "metadata": {},
   "source": [
    "### Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6725b3-e87a-44ac-a122-9eb764ccd755",
   "metadata": {},
   "source": [
    "#### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11827571-a252-4f19-a81a-9e179e4e5458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 21:18:32.327406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-24 21:18:32.327488: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-24 21:18:32.327568: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Alderson): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c09c2-a1ec-4419-8441-f3872dc18ee4",
   "metadata": {},
   "source": [
    "#### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64496ba5-09e0-4dd6-b0b4-7a717293efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464c1d1-7781-4f0b-9e48-0e88b1854fce",
   "metadata": {},
   "source": [
    "#### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f763d7-dfe4-4474-8a01-6c6f022291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe665a-4bdd-48e9-ac6b-ae5d449f2632",
   "metadata": {},
   "source": [
    "#### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b71be37-d52d-4679-9649-ca245fc1ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5279b2-9d96-41d7-ad67-584e3684ccba",
   "metadata": {},
   "source": [
    "#### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52de354-e572-48ea-8ed3-2e011128b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0509e-68dc-4da1-88ba-df9a19f2f075",
   "metadata": {},
   "source": [
    "#### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2005b2-9010-4264-9413-64ebf128210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973277b4-e4a6-4503-a449-f54ab5d141bf",
   "metadata": {},
   "source": [
    "#### Step 5 - Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bfcb6f-99e9-448c-85e7-f9c97edefd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use sigmoid as activation function if we are doing binary classification or \n",
    "# other wise we use softmax if we have multiclass classification\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc025b9-6ddf-4969-811a-9218770612ce",
   "metadata": {},
   "source": [
    "### Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3db288-818f-4903-a5b4-d7f0c0b8c121",
   "metadata": {},
   "source": [
    "#### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14726549-29ee-43ea-ad76-7c8fe4894bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7aeaf2-6be2-48ee-a4da-4dbc34229d75",
   "metadata": {},
   "source": [
    "#### Training the CNN on the Training set and evaluating it on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d783c22-62e3-4597-ac34-500000f9ef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 21:18:42.573655: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15745024 exceeds 10% of free system memory.\n",
      "2022-01-24 21:18:44.069896: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28096128 exceeds 10% of free system memory.\n",
      "2022-01-24 21:18:44.177357: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15745024 exceeds 10% of free system memory.\n",
      "2022-01-24 21:18:44.223616: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 14530320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/250 [..............................] - ETA: 30:27 - loss: 0.7165 - accuracy: 0.4688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 21:18:45.044812: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15745024 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 358s 1s/step - loss: 0.6702 - accuracy: 0.5863 - val_loss: 0.6050 - val_accuracy: 0.6780\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 171s 682ms/step - loss: 0.5950 - accuracy: 0.6827 - val_loss: 0.5611 - val_accuracy: 0.7160\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 120s 481ms/step - loss: 0.5532 - accuracy: 0.7181 - val_loss: 0.5205 - val_accuracy: 0.7430\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 120s 479ms/step - loss: 0.5194 - accuracy: 0.7384 - val_loss: 0.4911 - val_accuracy: 0.7675\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 120s 480ms/step - loss: 0.4972 - accuracy: 0.7552 - val_loss: 0.4954 - val_accuracy: 0.7645\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 120s 481ms/step - loss: 0.4828 - accuracy: 0.7667 - val_loss: 0.4854 - val_accuracy: 0.7720\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 121s 482ms/step - loss: 0.4757 - accuracy: 0.7697 - val_loss: 0.4800 - val_accuracy: 0.7715\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 121s 482ms/step - loss: 0.4448 - accuracy: 0.7910 - val_loss: 0.5011 - val_accuracy: 0.7670\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 120s 481ms/step - loss: 0.4385 - accuracy: 0.7930 - val_loss: 0.4613 - val_accuracy: 0.7830\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 120s 478ms/step - loss: 0.4195 - accuracy: 0.8024 - val_loss: 0.4599 - val_accuracy: 0.7835\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 121s 482ms/step - loss: 0.4069 - accuracy: 0.8179 - val_loss: 0.5607 - val_accuracy: 0.7345\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 0.4059 - accuracy: 0.8095 - val_loss: 0.4639 - val_accuracy: 0.7845\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 124s 494ms/step - loss: 0.3889 - accuracy: 0.8204 - val_loss: 0.4825 - val_accuracy: 0.7770\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 0.3805 - accuracy: 0.8281 - val_loss: 0.4453 - val_accuracy: 0.7935\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 121s 482ms/step - loss: 0.3615 - accuracy: 0.8376 - val_loss: 0.4644 - val_accuracy: 0.7765\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 0.3523 - accuracy: 0.8431 - val_loss: 0.4581 - val_accuracy: 0.7990\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 121s 483ms/step - loss: 0.3411 - accuracy: 0.8464 - val_loss: 0.4504 - val_accuracy: 0.8105\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 121s 483ms/step - loss: 0.3300 - accuracy: 0.8534 - val_loss: 0.4384 - val_accuracy: 0.8055\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 0.3149 - accuracy: 0.8606 - val_loss: 0.4660 - val_accuracy: 0.8130\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 0.3014 - accuracy: 0.8700 - val_loss: 0.4713 - val_accuracy: 0.8100\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 122s 487ms/step - loss: 0.2933 - accuracy: 0.8740 - val_loss: 0.5244 - val_accuracy: 0.7905\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 121s 483ms/step - loss: 0.2746 - accuracy: 0.8825 - val_loss: 0.5625 - val_accuracy: 0.7795\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 121s 483ms/step - loss: 0.2730 - accuracy: 0.8829 - val_loss: 0.4937 - val_accuracy: 0.8030\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 0.2572 - accuracy: 0.8951 - val_loss: 0.5489 - val_accuracy: 0.7865\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 121s 483ms/step - loss: 0.2518 - accuracy: 0.8907 - val_loss: 0.4974 - val_accuracy: 0.8055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8b03f8b80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d21848-abf3-475f-a1d6-a79505a193e7",
   "metadata": {},
   "source": [
    "### Part 4 - Making a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a34aaeab-a9b5-44aa-a49b-2711ad70450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/could_be_a_cat_from_another_universe.jpeg', target_size=[64, 64, 3])\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1 :\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bea6c8fd-9eee-473a-8d4d-8c3a72ccaee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8777095-6240-41c0-832f-a89d94e2c087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
